{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declaration**: Most code of this work is from https://github.com/tensorlayer/seq2seq-chatbot. I changed some of data processing code and modified the model in order to make it work on my laptop. An important goal of this final project is to compare the impacts of pretrained embedding and trained-from-scratch embedding to the results. My work is mainly focused on introducing the GloVe embedding matrix, in which I have to modify the code from data prepratation and totally rewrite the embedding layer part of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION=100 # Available dimensions for 6B data is 50, 100, 200, 300\n",
    "data_directory = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(data_directory):\n",
    "    os.path.makedirs(data_directory)\n",
    "    \n",
    "glove_weights_file_path = os.path.join(data_directory, f'glove.6B.{EMBEDDING_DIMENSION}d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(glove_weights_file_path):\n",
    "    # Glove embedding weights can be downloaded from https://nlp.stanford.edu/projects/glove/\n",
    "    glove_fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    local_zip_file_path = os.path.join(data_directory, os.path.basename(glove_fallback_url))\n",
    "    if not os.path.isfile(local_zip_file_path):\n",
    "        print(f'Retreiving glove weights from {glove_fallback_url}')\n",
    "        urllib.request.urlretrieve(glove_fallback_url, local_zip_file_path)\n",
    "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
    "        print(f'Extracting glove weights from {local_zip_file_path}')\n",
    "        z.extractall(path=data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "glove_data_directory = '.'\n",
    "\n",
    "word2idx = { 'PAD': PAD_TOKEN } # dict so we can lookup indices for tokenising our text later from string to sequence of integers\n",
    "weights = []\n",
    "\n",
    "with open (glove_data_directory + '/' +'glove.6B.100d.txt', 'r') as file:\n",
    "    for index, line in enumerate(file):\n",
    "        values = line.split() # Word and weights separated by space\n",
    "        word = values[0] # Word is first symbol on each line\n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word\n",
    "        word2idx[word] = index + 1 # PAD is our zeroth index so shift by one\n",
    "        weights.append(word_weights)\n",
    "        \n",
    "        if index + 1 == 40_000:\n",
    "            # Limit vocabulary to top 40k terms\n",
    "            break\n",
    "\n",
    "EMBEDDING_DIMENSION = len(weights[0])\n",
    "# Insert the PAD weights at index 0 now we know the embedding dimension\n",
    "weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Append unknown and pad to end of vocab and initialize as random\n",
    "UNKNOWN_TOKEN = len(weights)\n",
    "word2idx['UNK'] = UNKNOWN_TOKEN\n",
    "weights.append(np.random.randn(EMBEDDING_DIMENSION))\n",
    "\n",
    "# Construct our final vocab\n",
    "weights = np.asarray(weights, dtype=np.float32)\n",
    "\n",
    "VOCAB_SIZE = weights.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
    "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''\n",
    "\n",
    "limit = {\n",
    "        'maxq' : 25,\n",
    "        'minq' : 2,\n",
    "        'maxa' : 25,\n",
    "        'mina' : 2\n",
    "        }\n",
    "UNK = 'UNK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    1. Read from 'movie-lines.txt'\n",
    "    2. Create a dictionary with ( key = line_id, value = text )\n",
    "'''\n",
    "def get_id2line():\n",
    "    lines=open('../../../nlp_data/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    id2line = {}\n",
    "    for line in lines:\n",
    "        _line = line.split(' +++$+++ ')\n",
    "        if len(_line) == 5:\n",
    "            id2line[_line[0]] = _line[4]\n",
    "    return id2line\n",
    "\n",
    "'''\n",
    "    1. Read from 'movie_conversations.txt'\n",
    "    2. Create a list of [list of line_id's]\n",
    "'''\n",
    "def get_conversations():\n",
    "    conv_lines = open('../../../nlp_data/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "    convs = [ ]\n",
    "    for line in conv_lines[:-1]:\n",
    "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "        convs.append(_line.split(','))\n",
    "    return convs\n",
    "\n",
    "'''\n",
    "    1. Get each conversation\n",
    "    2. Get each line from conversation\n",
    "    3. Save each conversation to file\n",
    "'''\n",
    "def extract_conversations(convs,id2line,path=''):\n",
    "    idx = 0\n",
    "    for conv in convs:\n",
    "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
    "        for line_id in conv:\n",
    "            f_conv.write(id2line[line_id])\n",
    "            f_conv.write('\\n')\n",
    "        f_conv.close()\n",
    "        idx += 1\n",
    "\n",
    "'''\n",
    "    Get lists of all conversations as Questions and Answers\n",
    "    1. [questions]\n",
    "    2. [answers]\n",
    "'''\n",
    "def gather_dataset(convs, id2line):\n",
    "    questions = []; answers = []\n",
    "\n",
    "    for conv in convs:\n",
    "        if len(conv) %2 != 0:\n",
    "            conv = conv[:-1]\n",
    "        for i in range(len(conv)):\n",
    "            if i%2 == 0:\n",
    "                questions.append(id2line[conv[i]])\n",
    "            else:\n",
    "                answers.append(id2line[conv[i]])\n",
    "\n",
    "    return questions, answers\n",
    "\n",
    "\n",
    "'''\n",
    "    We need 4 files\n",
    "    1. train.enc : Encoder input for training\n",
    "    2. train.dec : Decoder input for training\n",
    "    3. test.enc  : Encoder input for testing\n",
    "    4. test.dec  : Decoder input for testing\n",
    "'''\n",
    "def prepare_seq2seq_files(questions, answers, path='',TESTSET_SIZE = 30000):\n",
    "\n",
    "    # open files\n",
    "    train_enc = open(path + 'train.enc','w')\n",
    "    train_dec = open(path + 'train.dec','w')\n",
    "    test_enc  = open(path + 'test.enc', 'w')\n",
    "    test_dec  = open(path + 'test.dec', 'w')\n",
    "\n",
    "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
    "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
    "\n",
    "    for i in range(len(questions)):\n",
    "        if i in test_ids:\n",
    "            test_enc.write(questions[i]+'\\n')\n",
    "            test_dec.write(answers[i]+ '\\n' )\n",
    "        else:\n",
    "            train_enc.write(questions[i]+'\\n')\n",
    "            train_dec.write(answers[i]+ '\\n' )\n",
    "        if i%10000 == 0:\n",
    "            print('\\n>> written {} lines'.format(i))\n",
    "\n",
    "    # close files\n",
    "    train_enc.close()\n",
    "    train_dec.close()\n",
    "    test_enc.close()\n",
    "    test_dec.close()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " remove anything that isn't in the vocabulary\n",
    "    return str(pure en)\n",
    "'''\n",
    "def filter_line(line, whitelist):\n",
    "    return ''.join([ ch for ch in line if ch in whitelist ])\n",
    "\n",
    "def add_space_punct(line):\n",
    "    return line.translate(str.maketrans({key: \" {0} \".format(key) for key in string.punctuation}))\n",
    "\n",
    "def replace_didt(line):\n",
    "    return re.sub(\"\\sdidn't\\s\", ' did not ', line)\n",
    "\n",
    "\n",
    "'''\n",
    " filter too long and too short sequences\n",
    "    return tuple( filtered_ta, filtered_en )\n",
    "'''\n",
    "def filter_data(qseq, aseq):\n",
    "    filtered_q, filtered_a = [], []\n",
    "    raw_data_len = len(qseq)\n",
    "\n",
    "    assert len(qseq) == len(aseq)\n",
    "\n",
    "    for i in range(raw_data_len):\n",
    "        qlen, alen = len(qseq[i].split()), len(aseq[i].split())\n",
    "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
    "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
    "                filtered_q.append(qseq[i])\n",
    "                filtered_a.append(aseq[i])\n",
    "\n",
    "    # print the fraction of the original data, filtered\n",
    "    filt_data_len = len(filtered_q)\n",
    "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
    "    print(str(filtered) + '% filtered from original data')\n",
    "\n",
    "    return filtered_q, filtered_a\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " read list of words, create index to word,\n",
    "  word to index dictionaries\n",
    "    return tuple( vocab->(word, count), idx2w, w2idx )\n",
    "'''\n",
    "def index_(tokenized_sentences, vocab_size):\n",
    "    # get frequency distribution\n",
    "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    # get vocabulary of 'vocab_size' most used words\n",
    "    vocab = freq_dist.most_common(vocab_size)\n",
    "    # index2word\n",
    "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
    "    # word2index\n",
    "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
    "    return index2word, word2index, freq_dist\n",
    "\n",
    "'''\n",
    " filter based on number of unknowns (words not in vocabulary)\n",
    "  filter out the worst sentences\n",
    "'''\n",
    "def filter_unk(qtokenized, atokenized, w2idx):\n",
    "    data_len = len(qtokenized)\n",
    "\n",
    "    filtered_q, filtered_a = [], []\n",
    "\n",
    "    for qline, aline in zip(qtokenized, atokenized):\n",
    "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
    "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
    "        if unk_count_a <= 2:\n",
    "            if unk_count_q > 0:\n",
    "                if unk_count_q/len(qline) > 0.2:\n",
    "                    pass\n",
    "            filtered_q.append(qline)\n",
    "            filtered_a.append(aline)\n",
    "\n",
    "    # print the fraction of the original data, filtered\n",
    "    filt_data_len = len(filtered_q)\n",
    "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
    "    print(str(filtered) + '% filtered from original data')\n",
    "\n",
    "    return filtered_q, filtered_a\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    " create the final dataset :\n",
    "  - convert list of items to arrays of indices\n",
    "  - add zero padding\n",
    "      return ( [array_en([indices]), array_ta([indices]) )\n",
    "'''\n",
    "def zero_pad(qtokenized, atokenized, w2idx):\n",
    "    # num of rows\n",
    "    data_len = len(qtokenized)\n",
    "\n",
    "    # numpy arrays to store indices\n",
    "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32)\n",
    "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
    "\n",
    "    for i in range(data_len):\n",
    "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
    "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
    "\n",
    "        #print(len(idx_q[i]), len(q_indices))\n",
    "        #print(len(idx_a[i]), len(a_indices))\n",
    "        idx_q[i] = np.array(q_indices)\n",
    "        idx_a[i] = np.array(a_indices)\n",
    "\n",
    "    return idx_q, idx_a\n",
    "\n",
    "\n",
    "'''\n",
    " replace words with indices in a sequence\n",
    "  replace with unknown if word not in lookup\n",
    "    return [list of indices]\n",
    "'''\n",
    "def pad_seq(seq, lookup, maxlen):\n",
    "    indices = []\n",
    "    for word in seq:\n",
    "        if word in lookup:\n",
    "            indices.append(lookup[word])\n",
    "        else:\n",
    "            indices.append(lookup[UNK])\n",
    "    return indices + [0]*(maxlen - len(seq))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "'''\n",
    " split data into train (70%), test (15%) and valid(15%)\n",
    "    return tuple( (trainX, trainY), (testX,testY), (validX,validY) )\n",
    "'''\n",
    "def split_dataset(x, y, ratio = [0.7, 0.15, 0.15] ):\n",
    "    # number of examples\n",
    "    data_len = len(x)\n",
    "    lens = [ int(data_len*item) for item in ratio ]\n",
    "\n",
    "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
    "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
    "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
    "\n",
    "    return (trainX,trainY), (testX,testY), (validX,validY)\n",
    "\n",
    "\n",
    "'''\n",
    " generate batches from dataset\n",
    "    yield (x_gen, y_gen)\n",
    "    TODO : fix needed\n",
    "'''\n",
    "def batch_gen(x, y, batch_size):\n",
    "    # infinite while\n",
    "    while True:\n",
    "        for i in range(0, len(x), batch_size):\n",
    "            if (i+1)*batch_size < len(x):\n",
    "                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T\n",
    "\n",
    "'''\n",
    " generate batches, by random sampling a bunch of items\n",
    "    yield (x_gen, y_gen)\n",
    "'''\n",
    "def rand_batch_gen(x, y, batch_size):\n",
    "    while True:\n",
    "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
    "        yield x[sample_idx].T, y[sample_idx].T\n",
    "\n",
    "\n",
    "'''\n",
    " a generic decode function\n",
    "    inputs : sequence, lookup\n",
    "'''\n",
    "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
    "    return separator.join([ lookup[element] for element in sequence if element ])\n",
    "\n",
    "def load_data(PATH=''):\n",
    "    # read data control dictionaries\n",
    "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    # read numpy arrays\n",
    "    idx_q = np.load(PATH + 'idx_q.npy')\n",
    "    idx_a = np.load(PATH + 'idx_a.npy')\n",
    "    return metadata, idx_q, idx_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26% filtered from original data\n"
     ]
    }
   ],
   "source": [
    "id2line = get_id2line()\n",
    "convs = get_conversations()\n",
    "questions, answers = gather_dataset(convs,id2line)\n",
    "\n",
    "# change to lower case (just for en)\n",
    "questions = [ line.lower() for line in questions ]\n",
    "answers = [ line.lower() for line in answers ]\n",
    "\n",
    "# replace didn't to did not\n",
    "questions = [ replace_didt(line) for line in questions ]\n",
    "answers = [ replace_didt(line) for line in answers ]\n",
    "\n",
    "# pad punctuation\n",
    "questions = [ add_space_punct(line) for line in questions ]\n",
    "answers = [ add_space_punct(line) for line in answers ]\n",
    "\n",
    "# filter out too long or too short sequences\n",
    "qlines, alines = filter_data(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q : [pick you up friday ,  then]; a : [oh ,  right .   friday . ]\n",
      "q : [the night i take you to places you ' ve never been before .   and back . ]; a : [like where ?   the 7 - eleven on burnside ?  do you even know my name ,  screwboy ? ]\n",
      "q : [you hate me don ' t you ? ]; a : [i don ' t really think you warrant that strong an emotion . ]\n",
      "q : [then say you ' ll spend dollar night at the track with me . ]; a : [and why would i do that ? ]\n"
     ]
    }
   ],
   "source": [
    "for q,a in zip(qlines[141:145], alines[141:145]):\n",
    "    print('q : [{0}]; a : [{1}]'.format(q,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> Segment lines into words\n"
     ]
    }
   ],
   "source": [
    "# convert list of [lines of text] into list of [list of words ]\n",
    "print('\\n>> Segment lines into words')\n",
    "qtokenized = [ [w.strip() for w in wordlist.split() if w] for wordlist in qlines ]\n",
    "atokenized = [ [w.strip() for w in wordlist.split() if w] for wordlist in alines ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " >> Filter Unknowns\n",
      "0% filtered from original data\n",
      "\n",
      " Final dataset len : 100684\n",
      "\n",
      " >> Zero Padding\n",
      "\n",
      " >> Save numpy arrays to disk\n",
      "% unknown : 0.7643293233365639\n",
      "Dataset count : 100684\n"
     ]
    }
   ],
   "source": [
    "# filter out sentences with too many unknowns\n",
    "print('\\n >> Filter Unknowns')\n",
    "qtokenized, atokenized = filter_unk(qtokenized, atokenized, word2idx)\n",
    "print('\\n Final dataset len : ' + str(len(qtokenized)))\n",
    "print('\\n >> Zero Padding')\n",
    "idx_q, idx_a = zero_pad(qtokenized, atokenized, word2idx)\n",
    "print('\\n >> Save numpy arrays to disk')\n",
    "# save them\n",
    "np.save('idx_q.npy', idx_q)\n",
    "np.save('idx_a.npy', idx_a)\n",
    "\n",
    "# let us now save the necessary dictionaries\n",
    "metadata = {\n",
    "        'w2idx' : word2idx,\n",
    "#         'idx2w' : idx2w,\n",
    "        'limit' : limit\n",
    "#         'freq_dist' : freq_dist\n",
    "            }\n",
    "\n",
    "# write to disk : data control dictionaries\n",
    "with open('metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "# count of unknowns\n",
    "unk_count = (idx_q == UNKNOWN_TOKEN).sum() + (idx_a == UNKNOWN_TOKEN).sum()\n",
    "# count of words\n",
    "word_count = (idx_q != UNKNOWN_TOKEN).sum() + (idx_a != UNKNOWN_TOKEN).sum()\n",
    "\n",
    "print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
    "print('Dataset count : ' + str(idx_q.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata, idx_q, idx_a = load_data(PATH='./')          # Cornell Moive\n",
    "(trainX, trainY), (testX, testY), (validX, validY) = split_dataset(idx_q, idx_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.tolist()\n",
    "trainY = trainY.tolist()\n",
    "testX = testX.tolist()\n",
    "testY = testY.tolist()\n",
    "validX = validX.tolist()\n",
    "validY = validY.tolist()\n",
    "\n",
    "trainX = tl.prepro.remove_pad_sequences(trainX)\n",
    "trainY = tl.prepro.remove_pad_sequences(trainY)\n",
    "testX = tl.prepro.remove_pad_sequences(testX)\n",
    "testY = tl.prepro.remove_pad_sequences(testY)\n",
    "validX = tl.prepro.remove_pad_sequences(validX)\n",
    "validY = tl.prepro.remove_pad_sequences(validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = list(word2idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xseq_len = len(trainX)#.shape[-1]\n",
    "yseq_len = len(trainY)#.shape[-1]\n",
    "assert xseq_len == yseq_len\n",
    "batch_size = 32\n",
    "n_step = int(xseq_len/batch_size)\n",
    "emb_dim = 100 \n",
    "\n",
    "unk_id = word2idx['UNK']   \n",
    "pad_id = word2idx['PAD']     \n",
    "\n",
    "start_id = VOCAB_LENGTH \n",
    "end_id = VOCAB_LENGTH + 1\n",
    "\n",
    "word2idx.update({'start_id': start_id})\n",
    "word2idx.update({'end_id': end_id})\n",
    "idx2word = idx2word + ['start_id', 'end_id']\n",
    "xvocab_size = len(idx2word)\n",
    "\n",
    "VOCAB_LENGTH = VOCAB_LENGTH + 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode_seqs ['hi', '.']\n",
      "target_seqs ['looks', 'like', 'things', 'worked', 'out', 'tonight', ',', 'huh', '?', 'end_id']\n",
      "decode_seqs ['start_id', 'looks', 'like', 'things', 'worked', 'out', 'tonight', ',', 'huh', '?']\n",
      "target_mask [1 1 1 1 1 1 1 1 1 1]\n",
      "10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(\"encode_seqs\", [idx2word[id] for id in trainX[10]])\n",
    "target_seqs = tl.prepro.sequences_add_end_id([trainY[10]], end_id=end_id)[0]\n",
    "    # target_seqs = tl.prepro.remove_pad_sequences([target_seqs], pad_id=pad_id)[0]\n",
    "print(\"target_seqs\", [idx2word[id] for id in target_seqs])\n",
    "decode_seqs = tl.prepro.sequences_add_start_id([trainY[10]], start_id=start_id, remove_last=False)[0]\n",
    "    # decode_seqs = tl.prepro.remove_pad_sequences([decode_seqs], pad_id=pad_id)[0]\n",
    "print(\"decode_seqs\", [idx2word[id] for id in decode_seqs])\n",
    "target_mask = tl.prepro.sequences_get_mask([target_seqs])[0]\n",
    "print(\"target_mask\", target_mask)\n",
    "print(len(target_seqs), len(decode_seqs), len(target_mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###============= model\n",
    "def model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        # for chatbot, you can use the same embedding layer,\n",
    "        # for translation, you may want to use 2 seperated embedding layers\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            net_encode = EmbeddingInputlayer(\n",
    "                inputs = encode_seqs,\n",
    "                vocabulary_size = xvocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'seq_embedding')\n",
    "            vs.reuse_variables()\n",
    "            tl.layers.set_name_reuse(True) # remove if TL version == 1.8.0+\n",
    "            net_decode = EmbeddingInputlayer(\n",
    "                inputs = decode_seqs,\n",
    "                vocabulary_size = xvocab_size,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'seq_embedding')\n",
    "        net_rnn = Seq2Seq(net_encode, net_decode,\n",
    "                cell_fn = tf.contrib.rnn.BasicLSTMCell,\n",
    "                n_hidden = emb_dim,\n",
    "                initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "                encode_sequence_length = retrieve_seq_length_op2(encode_seqs),\n",
    "                decode_sequence_length = retrieve_seq_length_op2(decode_seqs),\n",
    "                initial_state_encode = None,\n",
    "                dropout = (0.5 if is_train else None),\n",
    "                n_layer = 3,\n",
    "                return_seq_2d = True,\n",
    "                name = 'seq2seq')\n",
    "        net_out = DenseLayer(net_rnn, n_units=xvocab_size, act=tf.identity, name='output')\n",
    "    return net_out, net_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_weights_initializer = tf.constant_initializer(weights)\n",
    "# embedding_weights = tf.get_variable(\n",
    "#     name='embedding_weights', \n",
    "#     shape=(VOCAB_LENGTH, EMBEDDING_DIMENSION), \n",
    "#     initializer=glove_weights_initializer,\n",
    "#     trainable=False)\n",
    "# embedding = tf.nn.embedding_lookup(embedding_weights, features['word_indices'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part is for testing the embedding input layer (don't run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, shape=[batch_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = tl.layers.EmbeddingInputlayer(inputs=x, vocabulary_size=VOCAB_LENGTH,\n",
    "                                                       embedding_size=emb_dim, E_init=glove_weights_initializer,\n",
    "                                                        E_init_args={'trainable': False},\n",
    "                                                       name='embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(aa.outputs, feed_dict={x: [20,30, 23, 1,2,3,4,5,6,7,78,8,9,9,5,4,3,2,2,2,23,3,4,5,2,3,3,3,4,5,6,7]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "###============= model\n",
    "def model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        # for chatbot, you can use the same embedding layer,\n",
    "        # for translation, you may want to use 2 seperated embedding layers\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            net_encode = tl.layers.EmbeddingInputlayer(inputs=encode_seqs, vocabulary_size=VOCAB_LENGTH,\n",
    "                                                       embedding_size=emb_dim, E_init=glove_weights_initializer,\n",
    "                                                        E_init_args={'trainable': False},\n",
    "                                                       name='embed')\n",
    "            vs.reuse_variables()\n",
    "            tl.layers.set_name_reuse(True) # remove if TL version == 1.8.0+\n",
    "            net_decode =tl.layers.EmbeddingInputlayer(inputs=decode_seqs, vocabulary_size=VOCAB_LENGTH,\n",
    "                                                      E_init=glove_weights_initializer, embedding_size=emb_dim,\n",
    "                                                      E_init_args={'trainable': False},\n",
    "                                                      name='embed')\n",
    "        net_rnn = Seq2Seq(net_encode, net_decode,\n",
    "                cell_fn = tf.contrib.rnn.BasicLSTMCell,\n",
    "                n_hidden = emb_dim,\n",
    "                initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "                encode_sequence_length = retrieve_seq_length_op2(encode_seqs),\n",
    "                decode_sequence_length = retrieve_seq_length_op2(decode_seqs),\n",
    "                initial_state_encode = None,\n",
    "                dropout = (0.5 if is_train else None),\n",
    "                n_layer = 3,\n",
    "                return_seq_2d = True,\n",
    "                name = 'seq2seq')\n",
    "        net_out = DenseLayer(net_rnn, n_units=xvocab_size, act=tf.identity, name='output')\n",
    "    return net_out, net_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] EmbeddingInputlayer model/embedding/embed: (40004, 100)\n",
      "[TL] WARNING: From <ipython-input-19-83ca66f147fc>:12: set_name_reuse (from tensorlayer.layers.utils) is deprecated and will be removed after 2018-06-30.\n",
      "Instructions for updating: TensorLayer relies on TensorFlow to check name reusing\n",
      "\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] EmbeddingInputlayer model/embedding/embed: (40004, 100)\n",
      "[TL] [*] Seq2Seq model/seq2seq: n_hidden: 100 cell_fn: BasicLSTMCell dropout: 0.5 n_layer: 3\n",
      "[TL] DynamicRNNLayer model/seq2seq/encode: n_hidden: 100, in_dim: 3 in_shape: (32, ?, 100) cell_fn: BasicLSTMCell dropout: 0.5 n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 32\n",
      "[TL] DynamicRNNLayer model/seq2seq/decode: n_hidden: 100, in_dim: 3 in_shape: (32, ?, 100) cell_fn: BasicLSTMCell dropout: 0.5 n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 32\n",
      "[TL] DenseLayer  model/output: 40004 No Activation\n",
      "[TL] EmbeddingInputlayer model/embedding/embed: (40004, 100)\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] EmbeddingInputlayer model/embedding/embed: (40004, 100)\n",
      "[TL] [*] Seq2Seq model/seq2seq: n_hidden: 100 cell_fn: BasicLSTMCell dropout: None n_layer: 3\n",
      "[TL] DynamicRNNLayer model/seq2seq/encode: n_hidden: 100, in_dim: 3 in_shape: (1, ?, 100) cell_fn: BasicLSTMCell dropout: None n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 1\n",
      "[TL] DynamicRNNLayer model/seq2seq/decode: n_hidden: 100, in_dim: 3 in_shape: (1, ?, 100) cell_fn: BasicLSTMCell dropout: None n_layer: 3\n",
      "[TL]        batch_size (concurrent processes): 1\n",
      "[TL] DenseLayer  model/output: 40004 No Activation\n",
      "[TL]   param   0: model/embedding/embed/embeddings:0 (40004, 100)       float32_ref\n",
      "[TL]   param   1: model/seq2seq/encode/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0 (200, 400)         float32_ref\n",
      "[TL]   param   2: model/seq2seq/encode/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0 (400,)             float32_ref\n",
      "[TL]   param   3: model/seq2seq/encode/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0 (200, 400)         float32_ref\n",
      "[TL]   param   4: model/seq2seq/encode/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0 (400,)             float32_ref\n",
      "[TL]   param   5: model/seq2seq/encode/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0 (200, 400)         float32_ref\n",
      "[TL]   param   6: model/seq2seq/encode/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0 (400,)             float32_ref\n",
      "[TL]   param   7: model/seq2seq/decode/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0 (200, 400)         float32_ref\n",
      "[TL]   param   8: model/seq2seq/decode/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0 (400,)             float32_ref\n",
      "[TL]   param   9: model/seq2seq/decode/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0 (200, 400)         float32_ref\n",
      "[TL]   param  10: model/seq2seq/decode/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0 (400,)             float32_ref\n",
      "[TL]   param  11: model/seq2seq/decode/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel:0 (200, 400)         float32_ref\n",
      "[TL]   param  12: model/seq2seq/decode/rnn/multi_rnn_cell/cell_2/basic_lstm_cell/bias:0 (400,)             float32_ref\n",
      "[TL]   param  13: model/output/W:0     (100, 40004)       float32_ref\n",
      "[TL]   param  14: model/output/b:0     (40004,)           float32_ref\n",
      "[TL]   num of params: 8523204\n",
      "[TL] WARNING: From <ipython-input-20-0e02b0cb7e90>:35: initialize_global_variables (from tensorlayer.layers.utils) is deprecated and will be removed after 2018-09-30.\n",
      "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
      "\n",
      "[TL] [*] Load n2.npz SUCCESS!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorlayer.layers.dense.base_dense.DenseLayer at 0x7fc330e00ba8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model for training\n",
    "with tf.device('/device:GPU:0'):\n",
    "    encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\")\n",
    "    decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\")\n",
    "    target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\")\n",
    "    target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") # tl.prepro.sequences_get_mask()\n",
    "net_out, _ = model(encode_seqs, decode_seqs, is_train=True, reuse=False)\n",
    "\n",
    "# model for inferencing\n",
    "with tf.device('/device:GPU:0'):\n",
    "    encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "    decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "net, net_rnn = model(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\n",
    "y = tf.nn.softmax(net.outputs)\n",
    "\n",
    "loss = tl.cost.cross_entropy_seq_with_mask(logits=net_out.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')\n",
    "\n",
    "net_out.print_params(False)\n",
    "# original was 0.0001\n",
    "lr = 0.0001\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))\n",
    "tl.layers.initialize_global_variables(sess)\n",
    "tl.files.load_and_assign_npz(sess=sess, name='n2.npz', network=net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/65] step:[0/2202] loss:3.664974 took:0.91226s\n",
      "Epoch[0/65] step:[200/2202] loss:3.815032 took:0.11539s\n",
      "Epoch[0/65] step:[400/2202] loss:4.102279 took:0.11165s\n",
      "Epoch[0/65] step:[600/2202] loss:3.887945 took:0.11302s\n",
      "Epoch[0/65] step:[800/2202] loss:4.050809 took:0.11463s\n",
      "Query > happy birthday have a nice day\n",
      " > i don ' t know .\n",
      " > i ' ll get you .\n",
      " > i don ' t know .\n",
      " > i don ' t want to be . . .\n",
      " > i don ' t know .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > i don ' t know , i ' m not going to get it .\n",
      " > the UNK . . .\n",
      " > i don ' t know .\n",
      "Epoch[0/65] step:[1000/2202] loss:3.798570 took:0.09714s\n",
      "Epoch[0/65] step:[1200/2202] loss:4.014073 took:0.11517s\n",
      "Epoch[0/65] step:[1400/2202] loss:3.820731 took:0.12036s\n",
      "Epoch[0/65] step:[1600/2202] loss:3.702559 took:0.10918s\n",
      "Epoch[0/65] step:[1800/2202] loss:3.670743 took:0.11417s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not going to be a UNK , i ' m sorry .\n",
      " > you ' re a little man . i ' m not going to get it .\n",
      " > you ' ll have a UNK .\n",
      " > i ' m sorry .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > it ' s not . . .\n",
      " > i ' m sorry .\n",
      " > i ' m not going to be a little .\n",
      " > i ' ll be in a little .\n",
      "Epoch[0/65] step:[2000/2202] loss:3.981548 took:0.10266s\n",
      "Epoch[0/65] step:[2200/2202] loss:3.864340 took:0.12060s\n",
      "Epoch[0/65] averaged loss:3.869068 took:249.89850s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[1/65] step:[0/2202] loss:4.111358 took:0.12041s\n",
      "Epoch[1/65] step:[200/2202] loss:4.008632 took:0.10347s\n",
      "Epoch[1/65] step:[400/2202] loss:3.912383 took:0.10591s\n",
      "Epoch[1/65] step:[600/2202] loss:3.868181 took:0.11548s\n",
      "Epoch[1/65] step:[800/2202] loss:4.095076 took:0.10789s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' ll have to be a little . . .\n",
      " > you ' re not going to get it .\n",
      " > you ' re not going to be a little man .\n",
      " > i ' m not going to get a UNK .\n",
      " > i ' m not sure .\n",
      "Query > how was it going\n",
      " > i ' m not .\n",
      " > it ' s a UNK . . .\n",
      " > i ' m sorry .\n",
      " > i ' m not . . . i don ' t know . . .\n",
      " > i don ' t know .\n",
      "Epoch[1/65] step:[1000/2202] loss:3.964360 took:0.10963s\n",
      "Epoch[1/65] step:[1200/2202] loss:4.041425 took:0.11473s\n",
      "Epoch[1/65] step:[1400/2202] loss:3.939144 took:0.10783s\n",
      "Epoch[1/65] step:[1600/2202] loss:3.841913 took:0.11828s\n",
      "Epoch[1/65] step:[1800/2202] loss:3.801182 took:0.11161s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i ' m not going to get a little of you . i ' ll get you to get a minute .\n",
      " > what ' s the difference ?\n",
      " > what ?\n",
      " > i don ' t want you .\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > i don ' t know . . . i don ' t know .\n",
      " > i ' m sorry .\n",
      " > the UNK . . . .\n",
      " > i don ' t know . . .\n",
      "Epoch[1/65] step:[2000/2202] loss:4.114898 took:0.11614s\n",
      "Epoch[1/65] step:[2200/2202] loss:3.975234 took:0.11319s\n",
      "Epoch[1/65] averaged loss:3.865692 took:248.96392s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[2/65] step:[0/2202] loss:3.934929 took:0.12483s\n",
      "Epoch[2/65] step:[200/2202] loss:3.742716 took:0.10871s\n",
      "Epoch[2/65] step:[400/2202] loss:3.698581 took:0.11523s\n",
      "Epoch[2/65] step:[600/2202] loss:3.609320 took:0.11417s\n",
      "Epoch[2/65] step:[800/2202] loss:4.178891 took:0.11286s\n",
      "Query > happy birthday have a nice day\n",
      " > what ' d you do ?\n",
      " > i ' m not going to get it .\n",
      " > what ' s the matter ?\n",
      " > i ' m not going to be a little .\n",
      " > you ' re not going to be a UNK .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i ' m not going to get a little of the UNK .\n",
      " > i ' ll be a little .\n",
      " > i don ' t know .\n",
      " > the one . . .\n",
      "Epoch[2/65] step:[1000/2202] loss:3.245116 took:0.11213s\n",
      "Epoch[2/65] step:[1200/2202] loss:3.819693 took:0.11900s\n",
      "Epoch[2/65] step:[1400/2202] loss:3.834815 took:0.11034s\n",
      "Epoch[2/65] step:[1600/2202] loss:3.508833 took:0.09769s\n",
      "Epoch[2/65] step:[1800/2202] loss:4.071410 took:0.12216s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i don ' t know .\n",
      " > i ' m not sure .\n",
      " > i ' ll get you . i ' m not going to be a little UNK , i can ' t get to be .\n",
      " > i ' m not going to be a little .\n",
      "Query > how was it going\n",
      " > the one .\n",
      " > i ' ll be a UNK .\n",
      " > the one of the world .\n",
      " > i ' m not going to get it .\n",
      " > i don ' t think , i ' ll be a UNK .\n",
      "Epoch[2/65] step:[2000/2202] loss:3.608158 took:0.11207s\n",
      "Epoch[2/65] step:[2200/2202] loss:3.819824 took:0.10529s\n",
      "Epoch[2/65] averaged loss:3.865499 took:249.09044s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[3/65] step:[0/2202] loss:4.011500 took:0.10672s\n",
      "Epoch[3/65] step:[200/2202] loss:3.693992 took:0.12337s\n",
      "Epoch[3/65] step:[400/2202] loss:3.816942 took:0.12249s\n",
      "Epoch[3/65] step:[600/2202] loss:4.189657 took:0.11782s\n",
      "Epoch[3/65] step:[800/2202] loss:4.208517 took:0.11679s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > you ' re not going to get a little of the UNK .\n",
      " > what ?\n",
      " > you ' re not going to be a UNK , UNK .\n",
      " > what ' s that ?\n",
      "Query > how was it going\n",
      " > the one of a UNK . . .\n",
      " > i ' m not going to get it .\n",
      " > the one .\n",
      " > i don ' t know .\n",
      " > i don ' t know , you ' re not a UNK .\n",
      "Epoch[3/65] step:[1000/2202] loss:4.221270 took:0.11640s\n",
      "Epoch[3/65] step:[1200/2202] loss:3.793322 took:0.11854s\n",
      "Epoch[3/65] step:[1400/2202] loss:3.617671 took:0.10245s\n",
      "Epoch[3/65] step:[1600/2202] loss:4.090024 took:0.11347s\n",
      "Epoch[3/65] step:[1800/2202] loss:4.223645 took:0.11795s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' ll be a UNK , UNK .\n",
      " > i ' ll get it out .\n",
      " > what ?\n",
      " > you ' re not a UNK , UNK . i ' m not going to be a little .\n",
      " > i ' m not sure , you ' re a good man .\n",
      "Query > how was it going\n",
      " > i ' ll be a little UNK .\n",
      " > i ' m sorry .\n",
      " > it was .\n",
      " > the UNK .\n",
      " > it was .\n",
      "Epoch[3/65] step:[2000/2202] loss:3.981947 took:0.11616s\n",
      "Epoch[3/65] step:[2200/2202] loss:3.730006 took:0.10603s\n",
      "Epoch[3/65] averaged loss:3.863606 took:249.31492s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[4/65] step:[0/2202] loss:4.236763 took:0.11505s\n",
      "Epoch[4/65] step:[200/2202] loss:3.804438 took:0.10942s\n",
      "Epoch[4/65] step:[400/2202] loss:3.840237 took:0.11307s\n",
      "Epoch[4/65] step:[600/2202] loss:3.882291 took:0.11386s\n",
      "Epoch[4/65] step:[800/2202] loss:3.825715 took:0.10821s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' ll be a UNK . . .\n",
      " > i ' ll be right .\n",
      " > i ' m sorry .\n",
      " > i ' m not going to be .\n",
      " > i ' ll get it .\n",
      "Query > how was it going\n",
      " > i ' ll get you .\n",
      " > it ' s not a good . . . i ' m not . . .\n",
      " > i ' m not going to get a UNK .\n",
      " > the UNK .\n",
      " > i ' m not going to be a little UNK .\n",
      "Epoch[4/65] step:[1000/2202] loss:3.648290 took:0.11861s\n",
      "Epoch[4/65] step:[1200/2202] loss:4.371187 took:0.10152s\n",
      "Epoch[4/65] step:[1400/2202] loss:3.587223 took:0.11610s\n",
      "Epoch[4/65] step:[1600/2202] loss:4.183870 took:0.11308s\n",
      "Epoch[4/65] step:[1800/2202] loss:3.938235 took:0.10650s\n",
      "Query > happy birthday have a nice day\n",
      " > what ' s the matter ?\n",
      " > what ?\n",
      " > i ' m not going .\n",
      " > i ' m not going to get it .\n",
      " > i don ' t know . i ' ll be a little UNK .\n",
      "Query > how was it going\n",
      " > the one of the UNK .\n",
      " > the UNK , UNK ' t it ?\n",
      " > i don ' t know .\n",
      " > i ' m sorry , i ' m not .\n",
      " > it ' s not a UNK .\n",
      "Epoch[4/65] step:[2000/2202] loss:3.690294 took:0.09654s\n",
      "Epoch[4/65] step:[2200/2202] loss:3.913456 took:0.11349s\n",
      "Epoch[4/65] averaged loss:3.862445 took:249.33388s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[5/65] step:[0/2202] loss:3.879066 took:0.12122s\n",
      "Epoch[5/65] step:[200/2202] loss:3.907912 took:0.11870s\n",
      "Epoch[5/65] step:[400/2202] loss:3.487457 took:0.10257s\n",
      "Epoch[5/65] step:[600/2202] loss:3.948286 took:0.12172s\n",
      "Epoch[5/65] step:[800/2202] loss:3.705498 took:0.11800s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to get it .\n",
      " > you ' re not a good UNK .\n",
      " > i ' ll get you to be a UNK .\n",
      " > what ?\n",
      " > i ' m sorry .\n",
      "Query > how was it going\n",
      " > the UNK . . .\n",
      " > i ' m sorry , i ' m not .\n",
      " > i ' m sorry .\n",
      " > i don ' t know .\n",
      " > it was .\n",
      "Epoch[5/65] step:[1000/2202] loss:3.955620 took:0.11239s\n",
      "Epoch[5/65] step:[1200/2202] loss:3.664873 took:0.11845s\n",
      "Epoch[5/65] step:[1400/2202] loss:3.980947 took:0.11308s\n",
      "Epoch[5/65] step:[1600/2202] loss:3.675321 took:0.10526s\n",
      "Epoch[5/65] step:[1800/2202] loss:3.455035 took:0.12220s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re a UNK . . .\n",
      " > you ' re not going to be a UNK , i ' ll get you .\n",
      " > i ' m sorry , you ' re not going .\n",
      " > what ?\n",
      " > i don ' t know .\n",
      "Query > how was it going\n",
      " > the UNK , UNK .\n",
      " > the UNK . . .\n",
      " > i don ' t know , i don ' t want to be . . .\n",
      " > the UNK .\n",
      " > the UNK .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[5/65] step:[2000/2202] loss:3.790045 took:0.12017s\n",
      "Epoch[5/65] step:[2200/2202] loss:3.754567 took:0.11691s\n",
      "Epoch[5/65] averaged loss:3.862785 took:249.16664s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[6/65] step:[0/2202] loss:3.780042 took:0.10063s\n",
      "Epoch[6/65] step:[200/2202] loss:3.617723 took:0.11164s\n",
      "Epoch[6/65] step:[400/2202] loss:3.896684 took:0.10613s\n",
      "Epoch[6/65] step:[600/2202] loss:3.986490 took:0.11406s\n",
      "Epoch[6/65] step:[800/2202] loss:3.823431 took:0.11175s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not going to be a little .\n",
      " > you ' re not a UNK .\n",
      " > you ' re not going to be a UNK .\n",
      " > you ' re not going to be a UNK .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > i ' ll be a little UNK .\n",
      " > it ' s a UNK , i ' m sorry .\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      " > the UNK , i ' m not going .\n",
      "Epoch[6/65] step:[1000/2202] loss:3.601669 took:0.11531s\n",
      "Epoch[6/65] step:[1200/2202] loss:3.860496 took:0.10877s\n",
      "Epoch[6/65] step:[1400/2202] loss:3.916353 took:0.09829s\n",
      "Epoch[6/65] step:[1600/2202] loss:3.668556 took:0.11567s\n",
      "Epoch[6/65] step:[1800/2202] loss:3.916005 took:0.10446s\n",
      "Query > happy birthday have a nice day\n",
      " > i don ' t know . i don ' t know .\n",
      " > you don ' t know . i don ' t know .\n",
      " > what ?\n",
      " > i ' m not sure , you ' re not a UNK .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i ' m not going to be .\n",
      " > the UNK .\n",
      " > i ' m not going to be .\n",
      " > i don ' t know .\n",
      "Epoch[6/65] step:[2000/2202] loss:4.174032 took:0.09928s\n",
      "Epoch[6/65] step:[2200/2202] loss:4.390143 took:0.12141s\n",
      "Epoch[6/65] averaged loss:3.860782 took:248.80972s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[7/65] step:[0/2202] loss:3.693281 took:0.10837s\n",
      "Epoch[7/65] step:[200/2202] loss:3.771045 took:0.11199s\n",
      "Epoch[7/65] step:[400/2202] loss:4.278038 took:0.12023s\n",
      "Epoch[7/65] step:[600/2202] loss:3.813652 took:0.11609s\n",
      "Epoch[7/65] step:[800/2202] loss:4.032002 took:0.10991s\n",
      "Query > happy birthday have a nice day\n",
      " > you don ' t know , i ' m sorry .\n",
      " > i ' m not going to get a UNK .\n",
      " > what ' s that ?\n",
      " > i ' m sorry .\n",
      " > i don ' t know .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the one .\n",
      "Epoch[7/65] step:[1000/2202] loss:3.742251 took:0.11223s\n",
      "Epoch[7/65] step:[1200/2202] loss:3.705501 took:0.11349s\n",
      "Epoch[7/65] step:[1400/2202] loss:3.978997 took:0.10537s\n",
      "Epoch[7/65] step:[1600/2202] loss:3.634357 took:0.11489s\n",
      "Epoch[7/65] step:[1800/2202] loss:3.632612 took:0.11204s\n",
      "Query > happy birthday have a nice day\n",
      " > i don ' t know .\n",
      " > you ' re a good man , UNK . i ' m not going to be a UNK .\n",
      " > you ' ll be right .\n",
      " > i ' ll be right .\n",
      " > you ' re not a good UNK .\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > i ' m sorry .\n",
      " > it ' s a UNK .\n",
      " > the UNK .\n",
      " > i ' m not going to be .\n",
      "Epoch[7/65] step:[2000/2202] loss:3.830893 took:0.10375s\n",
      "Epoch[7/65] step:[2200/2202] loss:3.795758 took:0.11612s\n",
      "Epoch[7/65] averaged loss:3.860397 took:248.85060s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[8/65] step:[0/2202] loss:3.893432 took:0.10994s\n",
      "Epoch[8/65] step:[200/2202] loss:3.953101 took:0.11326s\n",
      "Epoch[8/65] step:[400/2202] loss:3.597538 took:0.10250s\n",
      "Epoch[8/65] step:[600/2202] loss:3.703169 took:0.11216s\n",
      "Epoch[8/65] step:[800/2202] loss:3.854787 took:0.11193s\n",
      "Query > happy birthday have a nice day\n",
      " > you don ' t know .\n",
      " > you ' ll have to get a UNK .\n",
      " > what ' s the matter ?\n",
      " > you ' re not going to get a UNK .\n",
      " > you ' re not a UNK .\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > the UNK .\n",
      " > it ' s a lot .\n",
      " > i don ' t know .\n",
      " > i don ' t know , i ' ll be in the way .\n",
      "Epoch[8/65] step:[1000/2202] loss:3.737258 took:0.11277s\n",
      "Epoch[8/65] step:[1200/2202] loss:4.024945 took:0.10755s\n",
      "Epoch[8/65] step:[1400/2202] loss:3.819825 took:0.11475s\n",
      "Epoch[8/65] step:[1600/2202] loss:4.097085 took:0.11333s\n",
      "Epoch[8/65] step:[1800/2202] loss:4.145723 took:0.11681s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not a UNK . . .\n",
      " > you ' re a UNK , UNK .\n",
      " > you don ' t want to be a little man .\n",
      " > i ' m not going to get it .\n",
      " > i don ' t know , i ' m sorry .\n",
      "Query > how was it going\n",
      " > the UNK , UNK .\n",
      " > i ' m sorry .\n",
      " > it ' s a lot .\n",
      " > the UNK , i ' m not . . .\n",
      " > i ' m sorry .\n",
      "Epoch[8/65] step:[2000/2202] loss:3.890636 took:0.11347s\n",
      "Epoch[8/65] step:[2200/2202] loss:4.128546 took:0.11099s\n",
      "Epoch[8/65] averaged loss:3.859660 took:249.65401s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[9/65] step:[0/2202] loss:3.952173 took:0.11343s\n",
      "Epoch[9/65] step:[200/2202] loss:3.837366 took:0.10663s\n",
      "Epoch[9/65] step:[400/2202] loss:4.128745 took:0.11565s\n",
      "Epoch[9/65] step:[600/2202] loss:3.288356 took:0.11304s\n",
      "Epoch[9/65] step:[800/2202] loss:3.845022 took:0.11493s\n",
      "Query > happy birthday have a nice day\n",
      " > you don ' t want to be able to be a little UNK .\n",
      " > what ?\n",
      " > what ?\n",
      " > i ' m not sure .\n",
      " > i ' m not sure , i don ' t know . . .\n",
      "Query > how was it going\n",
      " > the UNK ' s .\n",
      " > i don ' t think , i don ' t know .\n",
      " > it ' s not a good .\n",
      " > the one . . .\n",
      " > the UNK .\n",
      "Epoch[9/65] step:[1000/2202] loss:3.952101 took:0.10940s\n",
      "Epoch[9/65] step:[1200/2202] loss:3.747911 took:0.10378s\n",
      "Epoch[9/65] step:[1400/2202] loss:3.984214 took:0.11996s\n",
      "Epoch[9/65] step:[1600/2202] loss:3.835989 took:0.11017s\n",
      "Epoch[9/65] step:[1800/2202] loss:3.577866 took:0.11225s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i ' m not going to be a UNK .\n",
      " > i ' m not going to get it out .\n",
      " > you ' re a good man .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > the one .\n",
      " > it ' s not a UNK .\n",
      " > i ' m not going to be . . .\n",
      " > the UNK .\n",
      " > i don ' t know , i ' m not . . .\n",
      "Epoch[9/65] step:[2000/2202] loss:3.654781 took:0.11539s\n",
      "Epoch[9/65] step:[2200/2202] loss:3.811577 took:0.11741s\n",
      "Epoch[9/65] averaged loss:3.860253 took:249.83631s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[10/65] step:[0/2202] loss:3.761159 took:0.11726s\n",
      "Epoch[10/65] step:[200/2202] loss:4.019804 took:0.11657s\n",
      "Epoch[10/65] step:[400/2202] loss:3.932081 took:0.11674s\n",
      "Epoch[10/65] step:[600/2202] loss:3.185649 took:0.11708s\n",
      "Epoch[10/65] step:[800/2202] loss:3.903157 took:0.11064s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not going to get it . i ' m not going to get it .\n",
      " > what ?\n",
      " > i ' m sorry , i don ' t want to talk you .\n",
      " > i ' ll be a good of the UNK .\n",
      " > i ' m sorry , i ' m not sure .\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > i don ' t know , i ' m sorry .\n",
      " > the UNK .\n",
      " > i ' m not sure , but i ' m sorry .\n",
      " > it ' s not . . .\n",
      "Epoch[10/65] step:[1000/2202] loss:4.029844 took:0.11832s\n",
      "Epoch[10/65] step:[1200/2202] loss:3.662336 took:0.10427s\n",
      "Epoch[10/65] step:[1400/2202] loss:3.689650 took:0.11467s\n",
      "Epoch[10/65] step:[1600/2202] loss:3.950979 took:0.11192s\n",
      "Epoch[10/65] step:[1800/2202] loss:3.801807 took:0.11835s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > you ' re a good UNK .\n",
      " > what ' s that ?\n",
      " > i don ' t know .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > the UNK . . .\n",
      " > it ' s a UNK .\n",
      " > the UNK .\n",
      " > it ' s a UNK . . .\n",
      " > the UNK .\n",
      "Epoch[10/65] step:[2000/2202] loss:3.880454 took:0.11973s\n",
      "Epoch[10/65] step:[2200/2202] loss:4.124352 took:0.11908s\n",
      "Epoch[10/65] averaged loss:3.858224 took:249.28732s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[11/65] step:[0/2202] loss:3.968005 took:0.12212s\n",
      "Epoch[11/65] step:[200/2202] loss:3.864456 took:0.10843s\n",
      "Epoch[11/65] step:[400/2202] loss:4.170256 took:0.11673s\n",
      "Epoch[11/65] step:[600/2202] loss:3.646272 took:0.11724s\n",
      "Epoch[11/65] step:[800/2202] loss:3.740764 took:0.10888s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to get a little of the time .\n",
      " > you ' re a good UNK .\n",
      " > i ' m not sure .\n",
      " > you ' re not going to get it .\n",
      " > i ' m sorry , i ' m not sure .\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > i ' m not sure .\n",
      " > i ' m not going to get a UNK .\n",
      " > it ' s a lot .\n",
      " > the UNK , UNK .\n",
      "Epoch[11/65] step:[1000/2202] loss:3.703353 took:0.11036s\n",
      "Epoch[11/65] step:[1200/2202] loss:3.873480 took:0.11504s\n",
      "Epoch[11/65] step:[1400/2202] loss:4.201882 took:0.11599s\n",
      "Epoch[11/65] step:[1600/2202] loss:3.665376 took:0.12309s\n",
      "Epoch[11/65] step:[1800/2202] loss:3.716708 took:0.11068s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re a good man , i ' m sorry .\n",
      " > what ' s the matter ?\n",
      " > you ' re not a UNK .\n",
      " > what ' s the matter ?\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > it ' s not a good .\n",
      " > the UNK .\n",
      " > i don ' t know , i ' m not .\n",
      " > it ' s a UNK .\n",
      " > i ' m not .\n",
      "Epoch[11/65] step:[2000/2202] loss:3.939782 took:0.11337s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[11/65] step:[2200/2202] loss:3.870764 took:0.10632s\n",
      "Epoch[11/65] averaged loss:3.860251 took:249.22431s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[12/65] step:[0/2202] loss:4.058467 took:0.11452s\n",
      "Epoch[12/65] step:[200/2202] loss:3.688139 took:0.11110s\n",
      "Epoch[12/65] step:[400/2202] loss:3.804825 took:0.11146s\n",
      "Epoch[12/65] step:[600/2202] loss:3.846092 took:0.10244s\n",
      "Epoch[12/65] step:[800/2202] loss:3.774758 took:0.11497s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not a UNK .\n",
      " > you ' re a good man , i ' m sorry , i ' m not .\n",
      " > what ?\n",
      " > you ' re a UNK .\n",
      " > i don ' t know , i ' m sorry .\n",
      "Query > how was it going\n",
      " > it ' s not a good UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the one .\n",
      " > the UNK .\n",
      "Epoch[12/65] step:[1000/2202] loss:4.088830 took:0.11143s\n",
      "Epoch[12/65] step:[1200/2202] loss:3.552500 took:0.11755s\n",
      "Epoch[12/65] step:[1400/2202] loss:3.947554 took:0.11105s\n",
      "Epoch[12/65] step:[1600/2202] loss:3.780357 took:0.11254s\n",
      "Epoch[12/65] step:[1800/2202] loss:3.729454 took:0.11279s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > i ' ll get it .\n",
      " > what ' s that ?\n",
      " > i ' m sorry .\n",
      " > i ' m not going to be a UNK .\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > the UNK , i ' m not .\n",
      " > i don ' t know . . .\n",
      " > the UNK .\n",
      " > i ' m sorry .\n",
      "Epoch[12/65] step:[2000/2202] loss:3.486301 took:0.10824s\n",
      "Epoch[12/65] step:[2200/2202] loss:3.767778 took:0.11377s\n",
      "Epoch[12/65] averaged loss:3.857186 took:248.78012s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[13/65] step:[0/2202] loss:3.805022 took:0.10301s\n",
      "Epoch[13/65] step:[200/2202] loss:4.008799 took:0.11870s\n",
      "Epoch[13/65] step:[400/2202] loss:3.965261 took:0.11872s\n",
      "Epoch[13/65] step:[600/2202] loss:3.851749 took:0.11715s\n",
      "Epoch[13/65] step:[800/2202] loss:3.790772 took:0.11876s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > i don ' t know . i ' m sorry .\n",
      " > i don ' t know , you ' re not going to be .\n",
      " > i don ' t know , you ' re a good man .\n",
      " > i ' m sorry .\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > i don ' t know , i ' m not going .\n",
      " > the one . . .\n",
      " > it ' s a lot . . .\n",
      " > the UNK .\n",
      "Epoch[13/65] step:[1000/2202] loss:3.977837 took:0.11301s\n",
      "Epoch[13/65] step:[1200/2202] loss:3.847017 took:0.12068s\n",
      "Epoch[13/65] step:[1400/2202] loss:3.822563 took:0.10415s\n",
      "Epoch[13/65] step:[1600/2202] loss:4.188449 took:0.11252s\n",
      "Epoch[13/65] step:[1800/2202] loss:3.683108 took:0.11495s\n",
      "Query > happy birthday have a nice day\n",
      " > what ' s that ? i ' m not sure .\n",
      " > i ' m sorry .\n",
      " > what ?\n",
      " > i ' m sorry .\n",
      " > i ' m not going to be a UNK .\n",
      "Query > how was it going\n",
      " > the one .\n",
      " > the UNK .\n",
      " > the one .\n",
      " > i ' m sorry , i don ' t know .\n",
      " > the UNK , i ' m not going to get it . i ' m not sure . . .\n",
      "Epoch[13/65] step:[2000/2202] loss:4.072337 took:0.10606s\n",
      "Epoch[13/65] step:[2200/2202] loss:3.522070 took:0.11330s\n",
      "Epoch[13/65] averaged loss:3.858758 took:249.69692s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[14/65] step:[0/2202] loss:3.738666 took:0.11623s\n",
      "Epoch[14/65] step:[200/2202] loss:4.025853 took:0.11431s\n",
      "Epoch[14/65] step:[400/2202] loss:4.069058 took:0.11689s\n",
      "Epoch[14/65] step:[600/2202] loss:4.030849 took:0.10984s\n",
      "Epoch[14/65] step:[800/2202] loss:3.601553 took:0.09680s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not a good man .\n",
      " > i ' m not going to get it .\n",
      " > i ' m not going to get a UNK .\n",
      " > you don ' t want a lot .\n",
      " > i ' m sorry . i don ' t know .\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      " > i ' m not .\n",
      " > the UNK .\n",
      "Epoch[14/65] step:[1000/2202] loss:4.151059 took:0.11003s\n",
      "Epoch[14/65] step:[1200/2202] loss:4.104234 took:0.11786s\n",
      "Epoch[14/65] step:[1400/2202] loss:4.195333 took:0.11607s\n",
      "Epoch[14/65] step:[1600/2202] loss:3.764782 took:0.11311s\n",
      "Epoch[14/65] step:[1800/2202] loss:4.025436 took:0.11100s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re a good UNK .\n",
      " > i ' m not going to be .\n",
      " > what ?\n",
      " > i don ' t know .\n",
      " > you ' re a good man , UNK .\n",
      "Query > how was it going\n",
      " > i don ' t know . . . i ' m not . . .\n",
      " > i ' m not .\n",
      " > the UNK .\n",
      " > i ' m sorry . i don ' t know .\n",
      " > i ' m not going to get it .\n",
      "Epoch[14/65] step:[2000/2202] loss:3.814260 took:0.11268s\n",
      "Epoch[14/65] step:[2200/2202] loss:4.154092 took:0.11379s\n",
      "Epoch[14/65] averaged loss:3.857713 took:249.08300s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[15/65] step:[0/2202] loss:4.059222 took:0.11238s\n",
      "Epoch[15/65] step:[200/2202] loss:3.580434 took:0.11443s\n",
      "Epoch[15/65] step:[400/2202] loss:3.593431 took:0.10418s\n",
      "Epoch[15/65] step:[600/2202] loss:3.416397 took:0.10693s\n",
      "Epoch[15/65] step:[800/2202] loss:3.978829 took:0.11283s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to get it .\n",
      " > i don ' t know .\n",
      " > i ' m sorry .\n",
      " > i ' ll get it .\n",
      " > what ' s that ?\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > the one .\n",
      " > i ' m not going to be . . .\n",
      " > i don ' t know .\n",
      " > i ' m not going to get it .\n",
      "Epoch[15/65] step:[1000/2202] loss:3.972029 took:0.11355s\n",
      "Epoch[15/65] step:[1200/2202] loss:4.164512 took:0.12111s\n",
      "Epoch[15/65] step:[1400/2202] loss:4.472215 took:0.12171s\n",
      "Epoch[15/65] step:[1600/2202] loss:3.919686 took:0.11693s\n",
      "Epoch[15/65] step:[1800/2202] loss:3.989345 took:0.10557s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to get a little of the time .\n",
      " > i ' ll be a UNK .\n",
      " > you ' re a UNK .\n",
      " > you don ' t know , you ' re a good UNK .\n",
      " > i don ' t know , i ' ll be a UNK .\n",
      "Query > how was it going\n",
      " > the UNK , UNK ' s .\n",
      " > i ' m not sure .\n",
      " > the one .\n",
      " > i ' m sorry , sir . i ' m not going to get the UNK .\n",
      " > the UNK . . .\n",
      "Epoch[15/65] step:[2000/2202] loss:3.781644 took:0.11434s\n",
      "Epoch[15/65] step:[2200/2202] loss:3.775972 took:0.10162s\n",
      "Epoch[15/65] averaged loss:3.854494 took:249.30230s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[16/65] step:[0/2202] loss:3.858352 took:0.09702s\n",
      "Epoch[16/65] step:[200/2202] loss:4.097997 took:0.10096s\n",
      "Epoch[16/65] step:[400/2202] loss:3.865758 took:0.11277s\n",
      "Epoch[16/65] step:[600/2202] loss:3.632079 took:0.11655s\n",
      "Epoch[16/65] step:[800/2202] loss:3.890096 took:0.11449s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > you ' re not going to be a UNK , i ' m sorry .\n",
      " > i ' m not going to get a little .\n",
      " > i ' m not sure .\n",
      " > i ' ll be a good UNK , i ' m sorry .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i ' m not going to be .\n",
      " > i ' m not sure , i ' m not .\n",
      " > i don ' t know .\n",
      " > i ' m not sure .\n",
      "Epoch[16/65] step:[1000/2202] loss:4.138627 took:0.10830s\n",
      "Epoch[16/65] step:[1200/2202] loss:3.816497 took:0.11412s\n",
      "Epoch[16/65] step:[1400/2202] loss:3.586814 took:0.11359s\n",
      "Epoch[16/65] step:[1600/2202] loss:4.131373 took:0.11645s\n",
      "Epoch[16/65] step:[1800/2202] loss:3.709218 took:0.11855s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i ' m not going to get a UNK .\n",
      " > you ' re not a good UNK .\n",
      " > what ' s the matter ?\n",
      " > i don ' t know . i don ' t know .\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > the one .\n",
      " > i ' ll be in the way .\n",
      " > it ' s a lot of the UNK .\n",
      " > the one of a UNK . . .\n",
      "Epoch[16/65] step:[2000/2202] loss:3.613514 took:0.11718s\n",
      "Epoch[16/65] step:[2200/2202] loss:4.008439 took:0.11202s\n",
      "Epoch[16/65] averaged loss:3.856821 took:249.02493s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[17/65] step:[0/2202] loss:3.605379 took:0.11650s\n",
      "Epoch[17/65] step:[200/2202] loss:3.939481 took:0.10920s\n",
      "Epoch[17/65] step:[400/2202] loss:4.008763 took:0.11821s\n",
      "Epoch[17/65] step:[600/2202] loss:4.028928 took:0.12268s\n",
      "Epoch[17/65] step:[800/2202] loss:3.528527 took:0.11553s\n",
      "Query > happy birthday have a nice day\n",
      " > i don ' t know .\n",
      " > you ' ll be in the middle . . .\n",
      " > what ?\n",
      " > you ' re not going to be .\n",
      " > you ' re a UNK .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > i ' m not going to get it .\n",
      "Epoch[17/65] step:[1000/2202] loss:3.705076 took:0.11144s\n",
      "Epoch[17/65] step:[1200/2202] loss:3.576320 took:0.11438s\n",
      "Epoch[17/65] step:[1400/2202] loss:4.173170 took:0.11674s\n",
      "Epoch[17/65] step:[1600/2202] loss:3.906131 took:0.12298s\n",
      "Epoch[17/65] step:[1800/2202] loss:3.937887 took:0.11359s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > what ' s the difference ?\n",
      " > you ' re not going to be a UNK .\n",
      " > i ' m not going to get it .\n",
      " > i don ' t know , i ' m not going to be a little man .\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > i ' m sorry .\n",
      " > i ' m sorry .\n",
      " > i don ' t know .\n",
      " > i ' m not . . . i don ' t know .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[17/65] step:[2000/2202] loss:3.743751 took:0.11776s\n",
      "Epoch[17/65] step:[2200/2202] loss:3.577564 took:0.10563s\n",
      "Epoch[17/65] averaged loss:3.856284 took:249.23881s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[18/65] step:[0/2202] loss:3.876742 took:0.11508s\n",
      "Epoch[18/65] step:[200/2202] loss:4.006594 took:0.11732s\n",
      "Epoch[18/65] step:[400/2202] loss:3.979013 took:0.11693s\n",
      "Epoch[18/65] step:[600/2202] loss:4.172915 took:0.10982s\n",
      "Epoch[18/65] step:[800/2202] loss:3.706369 took:0.10623s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > what ' s the difference , you ' re a UNK .\n",
      " > you ' re not a good man .\n",
      " > i ' ll be right .\n",
      " > you ' re not a UNK .\n",
      "Query > how was it going\n",
      " > i ' m not going to be a UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > i don ' t know . . .\n",
      " > it ' s not a UNK . . .\n",
      "Epoch[18/65] step:[1000/2202] loss:3.653560 took:0.10366s\n",
      "Epoch[18/65] step:[1200/2202] loss:3.501284 took:0.11472s\n",
      "Epoch[18/65] step:[1400/2202] loss:4.006666 took:0.11854s\n",
      "Epoch[18/65] step:[1600/2202] loss:3.901409 took:0.11749s\n",
      "Epoch[18/65] step:[1800/2202] loss:3.641629 took:0.10227s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not sure , i ' m not .\n",
      " > what ?\n",
      " > i don ' t know .\n",
      " > you ' re a UNK .\n",
      " > i ' ll get it .\n",
      "Query > how was it going\n",
      " > the UNK , i ' m sorry .\n",
      " > the UNK .\n",
      " > i ' m not going to be a UNK . . .\n",
      " > i ' ll be in a UNK .\n",
      " > the UNK .\n",
      "Epoch[18/65] step:[2000/2202] loss:3.627601 took:0.11741s\n",
      "Epoch[18/65] step:[2200/2202] loss:3.991610 took:0.10997s\n",
      "Epoch[18/65] averaged loss:3.856538 took:248.56520s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[19/65] step:[0/2202] loss:3.717259 took:0.10822s\n",
      "Epoch[19/65] step:[200/2202] loss:3.961030 took:0.11962s\n",
      "Epoch[19/65] step:[400/2202] loss:3.981117 took:0.11499s\n",
      "Epoch[19/65] step:[600/2202] loss:3.580267 took:0.11804s\n",
      "Epoch[19/65] step:[800/2202] loss:3.672993 took:0.10469s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > i don ' t want to talk .\n",
      " > i ' ll get you to get the UNK .\n",
      " > you ' re a good man , UNK ' t it ?\n",
      " > you ' re not a good man .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > i ' m not going to be a little .\n",
      " > the UNK .\n",
      " > i ' m sorry .\n",
      "Epoch[19/65] step:[1000/2202] loss:3.950814 took:0.11834s\n",
      "Epoch[19/65] step:[1200/2202] loss:3.992485 took:0.11645s\n",
      "Epoch[19/65] step:[1400/2202] loss:3.943158 took:0.10501s\n",
      "Epoch[19/65] step:[1600/2202] loss:4.050787 took:0.11213s\n",
      "Epoch[19/65] step:[1800/2202] loss:3.955413 took:0.10810s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not going to be a UNK .\n",
      " > i ' m not going to be a UNK .\n",
      " > i ' m not sure .\n",
      " > i ' m not sure .\n",
      " > you ' re a UNK .\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > the one .\n",
      " > the UNK , UNK .\n",
      " > i ' m sorry . i ' m not a UNK .\n",
      " > the UNK .\n",
      "Epoch[19/65] step:[2000/2202] loss:3.569318 took:0.11680s\n",
      "Epoch[19/65] step:[2200/2202] loss:3.856104 took:0.11114s\n",
      "Epoch[19/65] averaged loss:3.854190 took:249.43736s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[20/65] step:[0/2202] loss:3.822044 took:0.11990s\n",
      "Epoch[20/65] step:[200/2202] loss:4.265165 took:0.11757s\n",
      "Epoch[20/65] step:[400/2202] loss:3.586793 took:0.10904s\n",
      "Epoch[20/65] step:[600/2202] loss:3.804645 took:0.11437s\n",
      "Epoch[20/65] step:[800/2202] loss:3.802482 took:0.11932s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > what ?\n",
      " > i ' ll be right .\n",
      " > you ' re not going to be a UNK .\n",
      " > i ' m not going to get it .\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > i ' m not going to be a little UNK .\n",
      " > i ' m not .\n",
      " > it ' s not a UNK .\n",
      " > the UNK .\n",
      "Epoch[20/65] step:[1000/2202] loss:4.269581 took:0.12102s\n",
      "Epoch[20/65] step:[1200/2202] loss:3.888680 took:0.11795s\n",
      "Epoch[20/65] step:[1400/2202] loss:3.758407 took:0.11903s\n",
      "Epoch[20/65] step:[1600/2202] loss:4.096146 took:0.13734s\n",
      "Epoch[20/65] step:[1800/2202] loss:4.150159 took:0.10071s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re a UNK .\n",
      " > you ' re a UNK , UNK .\n",
      " > you don ' t know .\n",
      " > what ?\n",
      " > i don ' t know , you ' re not going to be a UNK .\n",
      "Query > how was it going\n",
      " > it ' s a lot .\n",
      " > the UNK .\n",
      " > the one .\n",
      " > i ' m not going to be .\n",
      " > i ' m not going . . .\n",
      "Epoch[20/65] step:[2000/2202] loss:4.021053 took:0.11168s\n",
      "Epoch[20/65] step:[2200/2202] loss:3.799898 took:0.11835s\n",
      "Epoch[20/65] averaged loss:3.857044 took:251.06090s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[21/65] step:[0/2202] loss:3.834522 took:0.11188s\n",
      "Epoch[21/65] step:[200/2202] loss:4.063848 took:0.11578s\n",
      "Epoch[21/65] step:[400/2202] loss:3.543416 took:0.11187s\n",
      "Epoch[21/65] step:[600/2202] loss:3.936269 took:0.10976s\n",
      "Epoch[21/65] step:[800/2202] loss:3.558660 took:0.11171s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > you ' re not a good man .\n",
      " > you don ' t know .\n",
      " > i ' m sorry . i don ' t want to talk . i ' ll get it .\n",
      " > i ' m not sure .\n",
      "Query > how was it going\n",
      " > i ' m sorry .\n",
      " > it ' s a UNK .\n",
      " > i ' m not going to get a UNK .\n",
      " > i ' m not going to get it .\n",
      " > i ' ll be a UNK .\n",
      "Epoch[21/65] step:[1000/2202] loss:3.878100 took:0.11350s\n",
      "Epoch[21/65] step:[1200/2202] loss:3.945529 took:0.10406s\n",
      "Epoch[21/65] step:[1400/2202] loss:4.009196 took:0.11593s\n",
      "Epoch[21/65] step:[1600/2202] loss:3.653306 took:0.10007s\n",
      "Epoch[21/65] step:[1800/2202] loss:3.914122 took:0.10750s\n",
      "Query > happy birthday have a nice day\n",
      " > i don ' t know .\n",
      " > you ' re a UNK , UNK .\n",
      " > i don ' t want to be a UNK .\n",
      " > i ' m not sure .\n",
      " > i don ' t want to talk you .\n",
      "Query > how was it going\n",
      " > i don ' t know .\n",
      " > it ' s a UNK . . .\n",
      " > it ' s not a good UNK .\n",
      " > i don ' t know .\n",
      " > the UNK .\n",
      "Epoch[21/65] step:[2000/2202] loss:3.780955 took:0.11123s\n",
      "Epoch[21/65] step:[2200/2202] loss:4.078185 took:0.11594s\n",
      "Epoch[21/65] averaged loss:3.854831 took:249.25223s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[22/65] step:[0/2202] loss:4.292162 took:0.10539s\n",
      "Epoch[22/65] step:[200/2202] loss:3.651608 took:0.11011s\n",
      "Epoch[22/65] step:[400/2202] loss:3.865081 took:0.11447s\n",
      "Epoch[22/65] step:[600/2202] loss:3.951624 took:0.11477s\n",
      "Epoch[22/65] step:[800/2202] loss:3.956306 took:0.12201s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re a UNK .\n",
      " > what ' s that supposed to do about ?\n",
      " > i ' m not sure .\n",
      " > you ' ll have to be a UNK .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > the UNK . . .\n",
      " > i ' m not going to get a UNK .\n",
      " > i ' m sorry , i ' m sorry .\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      "Epoch[22/65] step:[1000/2202] loss:3.753488 took:0.11068s\n",
      "Epoch[22/65] step:[1200/2202] loss:3.869205 took:0.11988s\n",
      "Epoch[22/65] step:[1400/2202] loss:3.639037 took:0.10819s\n",
      "Epoch[22/65] step:[1600/2202] loss:3.865186 took:0.11183s\n",
      "Epoch[22/65] step:[1800/2202] loss:3.927860 took:0.11744s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i don ' t know , you know .\n",
      " > i ' m sorry .\n",
      " > i don ' t know , you ' re a good UNK .\n",
      " > i ' m sorry , i don ' t know .\n",
      "Query > how was it going\n",
      " > i ' ll be in a little of the UNK .\n",
      " > i ' m sorry .\n",
      " > i ' m not going to get it .\n",
      " > i don ' t know .\n",
      " > the UNK .\n",
      "Epoch[22/65] step:[2000/2202] loss:4.036555 took:0.10817s\n",
      "Epoch[22/65] step:[2200/2202] loss:3.864175 took:0.10037s\n",
      "Epoch[22/65] averaged loss:3.854988 took:248.80125s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[23/65] step:[0/2202] loss:4.358576 took:0.11363s\n",
      "Epoch[23/65] step:[200/2202] loss:3.888770 took:0.11994s\n",
      "Epoch[23/65] step:[400/2202] loss:3.996237 took:0.10394s\n",
      "Epoch[23/65] step:[600/2202] loss:3.798700 took:0.10927s\n",
      "Epoch[23/65] step:[800/2202] loss:3.856602 took:0.11238s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > i ' m sorry .\n",
      " > you ' re not a good man .\n",
      " > you ' re not going to be a little .\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > i don ' t know . . .\n",
      " > the UNK , i ' m sorry .\n",
      " > i ' m not going to be .\n",
      " > i ' m not going to get a UNK .\n",
      " > the UNK .\n",
      "Epoch[23/65] step:[1000/2202] loss:3.872921 took:0.11116s\n",
      "Epoch[23/65] step:[1200/2202] loss:3.399577 took:0.11940s\n",
      "Epoch[23/65] step:[1400/2202] loss:3.906756 took:0.11542s\n",
      "Epoch[23/65] step:[1600/2202] loss:4.354440 took:0.10263s\n",
      "Epoch[23/65] step:[1800/2202] loss:4.146811 took:0.10262s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' ll be a good UNK .\n",
      " > i ' m sorry .\n",
      " > you ' re not a UNK .\n",
      " > you ' re not a UNK .\n",
      " > i ' m not going to be a little .\n",
      "Query > how was it going\n",
      " > i ' ll be a little UNK . . .\n",
      " > i ' m not sure , i ' m not going to get a little of the time .\n",
      " > i ' m not sure , but you ' re a UNK .\n",
      " > it ' s a lot of a UNK .\n",
      " > i ' m sorry .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[23/65] step:[2000/2202] loss:3.710427 took:0.10627s\n",
      "Epoch[23/65] step:[2200/2202] loss:4.034122 took:0.11080s\n",
      "Epoch[23/65] averaged loss:3.854452 took:248.89103s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[24/65] step:[0/2202] loss:4.066487 took:0.10864s\n",
      "Epoch[24/65] step:[200/2202] loss:3.671767 took:0.11196s\n",
      "Epoch[24/65] step:[400/2202] loss:3.816467 took:0.11911s\n",
      "Epoch[24/65] step:[600/2202] loss:3.632607 took:0.11500s\n",
      "Epoch[24/65] step:[800/2202] loss:3.962574 took:0.10969s\n",
      "Query > happy birthday have a nice day\n",
      " > you don ' t want to be able . i ' m not sure .\n",
      " > i ' m not sure .\n",
      " > i don ' t know .\n",
      " > what ' s the matter ?\n",
      " > i don ' t want to talk .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      " > the UNK .\n",
      "Epoch[24/65] step:[1000/2202] loss:3.745245 took:0.11489s\n",
      "Epoch[24/65] step:[1200/2202] loss:3.788352 took:0.11197s\n",
      "Epoch[24/65] step:[1400/2202] loss:3.658774 took:0.12243s\n",
      "Epoch[24/65] step:[1600/2202] loss:3.712565 took:0.12244s\n",
      "Epoch[24/65] step:[1800/2202] loss:4.006292 took:0.11325s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' ll be right to the UNK .\n",
      " > i ' m sorry .\n",
      " > you ' re not going to be a UNK .\n",
      " > i don ' t want to be a UNK .\n",
      " > you ' re not a UNK , UNK .\n",
      "Query > how was it going\n",
      " > the UNK , UNK . . .\n",
      " > it ' s not a good UNK .\n",
      " > i don ' t know .\n",
      " > i ' m sorry .\n",
      " > i don ' t know .\n",
      "Epoch[24/65] step:[2000/2202] loss:3.857739 took:0.11400s\n",
      "Epoch[24/65] step:[2200/2202] loss:4.100680 took:0.11481s\n",
      "Epoch[24/65] averaged loss:3.853323 took:249.45017s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[25/65] step:[0/2202] loss:3.677321 took:0.11812s\n",
      "Epoch[25/65] step:[200/2202] loss:3.565158 took:0.11781s\n",
      "Epoch[25/65] step:[400/2202] loss:4.039661 took:0.10756s\n",
      "Epoch[25/65] step:[600/2202] loss:3.678844 took:0.10946s\n",
      "Epoch[25/65] step:[800/2202] loss:3.856167 took:0.09597s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not going to get a UNK , UNK ' t you ?\n",
      " > i ' m not sure .\n",
      " > you ' ll be right .\n",
      " > you ' re not going to be .\n",
      " > i don ' t know .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i ' m not going to get a little of the time .\n",
      " > it ' s a lot .\n",
      " > i don ' t know .\n",
      " > it was .\n",
      "Epoch[25/65] step:[1000/2202] loss:4.225978 took:0.10797s\n",
      "Epoch[25/65] step:[1200/2202] loss:3.939922 took:0.11897s\n",
      "Epoch[25/65] step:[1400/2202] loss:3.323609 took:0.11669s\n",
      "Epoch[25/65] step:[1600/2202] loss:4.109813 took:0.10965s\n",
      "Epoch[25/65] step:[1800/2202] loss:3.733179 took:0.11531s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to be a little . . .\n",
      " > you ' re not a good man . i ' m not going to get a UNK .\n",
      " > what ?\n",
      " > what ' s that ?\n",
      " > i ' m not going to be .\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > i ' m not sure .\n",
      " > the UNK , i ' ll be a UNK .\n",
      " > i ' m sorry . i ' m not going to get it .\n",
      " > the one of the world .\n",
      "Epoch[25/65] step:[2000/2202] loss:3.768200 took:0.11574s\n",
      "Epoch[25/65] step:[2200/2202] loss:4.120804 took:0.11532s\n",
      "Epoch[25/65] averaged loss:3.853980 took:249.43677s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[26/65] step:[0/2202] loss:3.846892 took:0.10421s\n",
      "Epoch[26/65] step:[200/2202] loss:3.900891 took:0.12061s\n",
      "Epoch[26/65] step:[400/2202] loss:3.837302 took:0.10483s\n",
      "Epoch[26/65] step:[600/2202] loss:3.807124 took:0.11730s\n",
      "Epoch[26/65] step:[800/2202] loss:3.852464 took:0.11719s\n",
      "Query > happy birthday have a nice day\n",
      " > i don ' t know . i don ' t know .\n",
      " > you ' re a good UNK , UNK ' t it ?\n",
      " > you don ' t know .\n",
      " > what ' s the difference ?\n",
      " > i ' m sorry .\n",
      "Query > how was it going\n",
      " > it was a UNK . . .\n",
      " > the one of the world . . .\n",
      " > the UNK .\n",
      " > i don ' t know .\n",
      " > the UNK .\n",
      "Epoch[26/65] step:[1000/2202] loss:3.788149 took:0.11632s\n",
      "Epoch[26/65] step:[1200/2202] loss:3.961089 took:0.11034s\n",
      "Epoch[26/65] step:[1400/2202] loss:3.750764 took:0.11725s\n",
      "Epoch[26/65] step:[1600/2202] loss:3.789545 took:0.11611s\n",
      "Epoch[26/65] step:[1800/2202] loss:3.891834 took:0.11622s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry , i ' ll be right .\n",
      " > you ' re not a good man .\n",
      " > you ' re a UNK , UNK ' t it ?\n",
      " > what ?\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > it ' s not . . .\n",
      " > the UNK .\n",
      " > i ' m not going to get it .\n",
      " > it ' s a lot of a UNK .\n",
      "Epoch[26/65] step:[2000/2202] loss:4.029484 took:0.11583s\n",
      "Epoch[26/65] step:[2200/2202] loss:3.801617 took:0.11391s\n",
      "Epoch[26/65] averaged loss:3.849789 took:249.86640s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[27/65] step:[0/2202] loss:3.787785 took:0.11012s\n",
      "Epoch[27/65] step:[200/2202] loss:4.012588 took:0.11309s\n",
      "Epoch[27/65] step:[400/2202] loss:3.745907 took:0.12050s\n",
      "Epoch[27/65] step:[600/2202] loss:4.063368 took:0.11970s\n",
      "Epoch[27/65] step:[800/2202] loss:3.739439 took:0.11206s\n",
      "Query > happy birthday have a nice day\n",
      " > you don ' t know .\n",
      " > i ' m not going to be a little man .\n",
      " > you ' re not going to get a UNK , you ' ll get it out .\n",
      " > what ?\n",
      " > what ?\n",
      "Query > how was it going\n",
      " > i ' m not sure .\n",
      " > the UNK .\n",
      " > i don ' t think , you ' ll have a UNK .\n",
      " > i ' ll get it .\n",
      " > the UNK .\n",
      "Epoch[27/65] step:[1000/2202] loss:4.016276 took:0.09895s\n",
      "Epoch[27/65] step:[1200/2202] loss:4.032200 took:0.11375s\n",
      "Epoch[27/65] step:[1400/2202] loss:3.773311 took:0.09615s\n",
      "Epoch[27/65] step:[1600/2202] loss:3.761715 took:0.11727s\n",
      "Epoch[27/65] step:[1800/2202] loss:3.805257 took:0.11445s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > what ?\n",
      " > what ' s that ?\n",
      " > i ' ll get you .\n",
      " > i ' m not going to be a little .\n",
      "Query > how was it going\n",
      " > i don ' t know . . .\n",
      " > i don ' t know . . .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      " > the UNK .\n",
      "Epoch[27/65] step:[2000/2202] loss:3.971224 took:0.11789s\n",
      "Epoch[27/65] step:[2200/2202] loss:3.990621 took:0.11326s\n",
      "Epoch[27/65] averaged loss:3.852568 took:249.57739s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[28/65] step:[0/2202] loss:4.274054 took:0.11152s\n",
      "Epoch[28/65] step:[200/2202] loss:3.892617 took:0.11062s\n",
      "Epoch[28/65] step:[400/2202] loss:3.632420 took:0.11903s\n",
      "Epoch[28/65] step:[600/2202] loss:3.857104 took:0.11739s\n",
      "Epoch[28/65] step:[800/2202] loss:3.947093 took:0.11163s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to be .\n",
      " > i ' ll be a UNK .\n",
      " > what ?\n",
      " > i ' m not going to be .\n",
      " > i ' m sorry .\n",
      "Query > how was it going\n",
      " > i ' ll get it .\n",
      " > the one .\n",
      " > i ' m sorry . i don ' t know .\n",
      " > i ' m sorry .\n",
      " > i ' m sorry .\n",
      "Epoch[28/65] step:[1000/2202] loss:4.046964 took:0.11467s\n",
      "Epoch[28/65] step:[1200/2202] loss:3.549781 took:0.10253s\n",
      "Epoch[28/65] step:[1400/2202] loss:3.627844 took:0.10489s\n",
      "Epoch[28/65] step:[1600/2202] loss:3.640209 took:0.11341s\n",
      "Epoch[28/65] step:[1800/2202] loss:3.689332 took:0.11356s\n",
      "Query > happy birthday have a nice day\n",
      " > you don ' t know , i don ' t know what i ' m doing to do .\n",
      " > you don ' t know .\n",
      " > what ?\n",
      " > i don ' t know .\n",
      " > you ' ll be a UNK .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i don ' t know , i ' m not going to get it .\n",
      " > it ' ll be a good .\n",
      " > i ' m not .\n",
      " > i don ' t know . . .\n",
      "Epoch[28/65] step:[2000/2202] loss:4.119797 took:0.11796s\n",
      "Epoch[28/65] step:[2200/2202] loss:4.141675 took:0.11518s\n",
      "Epoch[28/65] averaged loss:3.850860 took:249.46062s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[29/65] step:[0/2202] loss:3.797831 took:0.12399s\n",
      "Epoch[29/65] step:[200/2202] loss:3.801342 took:0.11250s\n",
      "Epoch[29/65] step:[400/2202] loss:3.617403 took:0.09085s\n",
      "Epoch[29/65] step:[600/2202] loss:3.867197 took:0.11180s\n",
      "Epoch[29/65] step:[800/2202] loss:4.080303 took:0.10838s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i ' ll get it .\n",
      " > i ' m sorry . i ' m not sure , i don ' t want to be .\n",
      " > what ' s the matter ?\n",
      " > i ' m not going to be .\n",
      "Query > how was it going\n",
      " > the one .\n",
      " > i don ' t know , you ' ll be in a little of the way . . .\n",
      " > the UNK .\n",
      " > i ' m not sure .\n",
      " > i ' m not sure .\n",
      "Epoch[29/65] step:[1000/2202] loss:3.857989 took:0.11421s\n",
      "Epoch[29/65] step:[1200/2202] loss:3.598819 took:0.12004s\n",
      "Epoch[29/65] step:[1400/2202] loss:3.960151 took:0.11293s\n",
      "Epoch[29/65] step:[1600/2202] loss:3.864764 took:0.10628s\n",
      "Epoch[29/65] step:[1800/2202] loss:3.920549 took:0.10957s\n",
      "Query > happy birthday have a nice day\n",
      " > what ' s that ?\n",
      " > i don ' t know , i don ' t know . . . i don ' t know what you want .\n",
      " > i ' m sorry .\n",
      " > i ' m not going to get a UNK .\n",
      " > i ' m not going to get a UNK .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i ' m not going to be .\n",
      " > i don ' t know .\n",
      " > it ' s a UNK .\n",
      " > i ' ll be a UNK .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[29/65] step:[2000/2202] loss:3.543935 took:0.11567s\n",
      "Epoch[29/65] step:[2200/2202] loss:3.696616 took:0.11405s\n",
      "Epoch[29/65] averaged loss:3.852783 took:249.38370s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[30/65] step:[0/2202] loss:3.559965 took:0.11733s\n",
      "Epoch[30/65] step:[200/2202] loss:3.584791 took:0.09599s\n",
      "Epoch[30/65] step:[400/2202] loss:4.087370 took:0.11158s\n",
      "Epoch[30/65] step:[600/2202] loss:3.698358 took:0.10300s\n",
      "Epoch[30/65] step:[800/2202] loss:4.002708 took:0.12215s\n",
      "Query > happy birthday have a nice day\n",
      " > you ' re not going to be a little man . i don ' t know .\n",
      " > what ?\n",
      " > you ' ll have a little UNK .\n",
      " > you ' re a good man .\n",
      " > i ' m not going to be .\n",
      "Query > how was it going\n",
      " > i ' m sorry , i ' m sorry .\n",
      " > it ' s a UNK .\n",
      " > i don ' t know , you ' re not going to get it .\n",
      " > the one .\n",
      " > i ' ll get you .\n",
      "Epoch[30/65] step:[1000/2202] loss:3.731169 took:0.10716s\n",
      "Epoch[30/65] step:[1200/2202] loss:3.769906 took:0.11054s\n",
      "Epoch[30/65] step:[1400/2202] loss:3.792693 took:0.12267s\n",
      "Epoch[30/65] step:[1600/2202] loss:4.258057 took:0.12088s\n",
      "Epoch[30/65] step:[1800/2202] loss:3.825488 took:0.09938s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to be a UNK .\n",
      " > you don ' t know .\n",
      " > i ' m sorry . i ' ll be right to the UNK .\n",
      " > i ' m not going to get a UNK .\n",
      " > i ' ll get it .\n",
      "Query > how was it going\n",
      " > the UNK . . . .\n",
      " > the UNK .\n",
      " > i ' m not going to be .\n",
      " > it ' s not a good .\n",
      " > i don ' t know .\n",
      "Epoch[30/65] step:[2000/2202] loss:3.481306 took:0.11263s\n",
      "Epoch[30/65] step:[2200/2202] loss:3.918368 took:0.12139s\n",
      "Epoch[30/65] averaged loss:3.849812 took:249.30976s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[31/65] step:[0/2202] loss:3.925350 took:0.11607s\n",
      "Epoch[31/65] step:[200/2202] loss:4.042412 took:0.11360s\n",
      "Epoch[31/65] step:[400/2202] loss:3.808602 took:0.10132s\n",
      "Epoch[31/65] step:[600/2202] loss:3.552917 took:0.10551s\n",
      "Epoch[31/65] step:[800/2202] loss:3.527102 took:0.11461s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > you ' re not going to get a UNK .\n",
      " > you ' re not a UNK , UNK ' t you ?\n",
      " > you ' re not going to get it . i ' ll be right .\n",
      " > i don ' t know .\n",
      "Query > how was it going\n",
      " > the UNK . . .\n",
      " > i don ' t know .\n",
      " > i ' m not sure .\n",
      " > it ' s a UNK . . .\n",
      " > the UNK , i don ' t think .\n",
      "Epoch[31/65] step:[1000/2202] loss:3.592193 took:0.12447s\n",
      "Epoch[31/65] step:[1200/2202] loss:3.826187 took:0.12046s\n",
      "Epoch[31/65] step:[1400/2202] loss:3.957853 took:0.11928s\n",
      "Epoch[31/65] step:[1600/2202] loss:4.025715 took:0.11296s\n",
      "Epoch[31/65] step:[1800/2202] loss:3.899738 took:0.11600s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' ll be a UNK .\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      " > i don ' t know .\n",
      " > i ' ll be a good of you .\n",
      "Query > how was it going\n",
      " > the UNK , i ' ll be in the UNK .\n",
      " > the UNK .\n",
      " > i don ' t know .\n",
      " > the UNK .\n",
      " > i don ' t know .\n",
      "Epoch[31/65] step:[2000/2202] loss:4.033217 took:0.11301s\n",
      "Epoch[31/65] step:[2200/2202] loss:3.866903 took:0.11604s\n",
      "Epoch[31/65] averaged loss:3.851955 took:249.42114s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[32/65] step:[0/2202] loss:3.952240 took:0.11420s\n",
      "Epoch[32/65] step:[200/2202] loss:3.822371 took:0.10361s\n",
      "Epoch[32/65] step:[400/2202] loss:4.005115 took:0.11276s\n",
      "Epoch[32/65] step:[600/2202] loss:3.915146 took:0.10267s\n",
      "Epoch[32/65] step:[800/2202] loss:3.800982 took:0.11050s\n",
      "Query > happy birthday have a nice day\n",
      " > what ' s the matter ?\n",
      " > you ' re not going to get a little of you , UNK ' t it ?\n",
      " > i don ' t want to talk you .\n",
      " > what ? !\n",
      " > you don ' t know .\n",
      "Query > how was it going\n",
      " > the UNK . . .\n",
      " > it ' s not a good UNK .\n",
      " > i ' ll be in a UNK .\n",
      " > the UNK .\n",
      " > i don ' t know .\n",
      "Epoch[32/65] step:[1000/2202] loss:3.928116 took:0.11041s\n",
      "Epoch[32/65] step:[1200/2202] loss:3.883176 took:0.11626s\n",
      "Epoch[32/65] step:[1400/2202] loss:3.736339 took:0.11337s\n",
      "Epoch[32/65] step:[1600/2202] loss:3.738215 took:0.11038s\n",
      "Epoch[32/65] step:[1800/2202] loss:4.258869 took:0.11690s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not sure .\n",
      " > you ' re not a good man .\n",
      " > i ' ll be right .\n",
      " > what ?\n",
      " > you don ' t want to be able to get a UNK .\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > i don ' t know .\n",
      " > the UNK .\n",
      " > i ' m sorry .\n",
      " > i ' m sorry , i ' m sorry .\n",
      "Epoch[32/65] step:[2000/2202] loss:3.840956 took:0.10485s\n",
      "Epoch[32/65] step:[2200/2202] loss:3.867162 took:0.11567s\n",
      "Epoch[32/65] averaged loss:3.851839 took:249.57223s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[33/65] step:[0/2202] loss:3.858869 took:0.11459s\n",
      "Epoch[33/65] step:[200/2202] loss:4.157403 took:0.11612s\n",
      "Epoch[33/65] step:[400/2202] loss:4.059820 took:0.11932s\n",
      "Epoch[33/65] step:[600/2202] loss:4.021153 took:0.11875s\n",
      "Epoch[33/65] step:[800/2202] loss:3.936714 took:0.10869s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > i ' m sorry . i ' m not going to get a little .\n",
      " > i ' m not sure .\n",
      " > i ' m not sure , i ' m sorry .\n",
      " > i ' m not going to get a UNK .\n",
      "Query > how was it going\n",
      " > it ' s not a good UNK , i ' m not .\n",
      " > i ' m not going to be a UNK . . .\n",
      " > i ' m sorry .\n",
      " > i ' m not going to get a UNK .\n",
      " > i don ' t know . . .\n",
      "Epoch[33/65] step:[1000/2202] loss:3.844162 took:0.11035s\n",
      "Epoch[33/65] step:[1200/2202] loss:3.698508 took:0.10964s\n",
      "Epoch[33/65] step:[1400/2202] loss:3.853879 took:0.11733s\n",
      "Epoch[33/65] step:[1600/2202] loss:3.886525 took:0.12132s\n",
      "Epoch[33/65] step:[1800/2202] loss:3.899062 took:0.11064s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m sorry .\n",
      " > what ?\n",
      " > what ' s the difference ?\n",
      " > you ' re a UNK , UNK .\n",
      " > i ' m not sure .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > i don ' t think , i ' m not .\n",
      " > the UNK .\n",
      " > it ' s a UNK .\n",
      " > i ' ll be a little UNK .\n",
      "Epoch[33/65] step:[2000/2202] loss:3.887425 took:0.10480s\n",
      "Epoch[33/65] step:[2200/2202] loss:4.115863 took:0.10682s\n",
      "Epoch[33/65] averaged loss:3.850007 took:249.19088s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[34/65] step:[0/2202] loss:3.822742 took:0.11584s\n",
      "Epoch[34/65] step:[200/2202] loss:3.817513 took:0.11065s\n",
      "Epoch[34/65] step:[400/2202] loss:3.932226 took:0.11097s\n",
      "Epoch[34/65] step:[600/2202] loss:4.061632 took:0.12003s\n",
      "Epoch[34/65] step:[800/2202] loss:3.312041 took:0.11194s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to be a UNK .\n",
      " > i ' m sorry , i ' m sorry , but you ' re not a UNK , i ' ll be a UNK .\n",
      " > i ' ll be right to the car .\n",
      " > i don ' t know .\n",
      " > what ' d you do ?\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > the UNK .\n",
      " > it ' s a lot of the UNK . . .\n",
      " > the UNK , i ' m not . . .\n",
      " > the UNK .\n",
      "Epoch[34/65] step:[1000/2202] loss:3.882782 took:0.11583s\n",
      "Epoch[34/65] step:[1200/2202] loss:4.035055 took:0.11911s\n",
      "Epoch[34/65] step:[1400/2202] loss:3.814705 took:0.10636s\n",
      "Epoch[34/65] step:[1600/2202] loss:3.590071 took:0.10449s\n",
      "Epoch[34/65] step:[1800/2202] loss:3.988143 took:0.11458s\n",
      "Query > happy birthday have a nice day\n",
      " > i ' m not going to get a little of the UNK .\n",
      " > you ' re not going to get a little .\n",
      " > what ' s the matter ?\n",
      " > what ?\n",
      " > i ' m sorry , you ' ll be a little man , UNK .\n",
      "Query > how was it going\n",
      " > the UNK .\n",
      " > the UNK . . .\n",
      " > i ' m not sure , i ' m sorry , i ' ll be a good UNK .\n",
      " > i ' m not going to get a little of the time .\n",
      " > the UNK .\n",
      "Epoch[34/65] step:[2000/2202] loss:4.266767 took:0.11535s\n",
      "Epoch[34/65] step:[2200/2202] loss:3.744524 took:0.11622s\n",
      "Epoch[34/65] averaged loss:3.851194 took:249.25479s\n",
      "[TL] [*] n2.npz saved\n",
      "Epoch[35/65] step:[0/2202] loss:4.033652 took:0.11562s\n",
      "Epoch[35/65] step:[200/2202] loss:3.645242 took:0.11424s\n",
      "Epoch[35/65] step:[400/2202] loss:4.012425 took:0.11726s\n",
      "Epoch[35/65] step:[600/2202] loss:3.902248 took:0.11655s\n",
      "Epoch[35/65] step:[800/2202] loss:3.643200 took:0.08874s\n",
      "Query > happy birthday have a nice day\n",
      " > what ?\n",
      " > what ?\n",
      " > i don ' t know .\n",
      " > you don ' t know , i ' ll be a UNK .\n",
      " > i ' m not going to get it .\n",
      "Query > how was it going\n",
      " > it ' s a UNK .\n",
      " > i ' m not going to be a little UNK .\n",
      " > i don ' t know .\n",
      " > i don ' t think , i don ' t want any .\n",
      " > it ' s not a good UNK .\n",
      "Epoch[35/65] step:[1000/2202] loss:3.920540 took:0.11295s\n",
      "Epoch[35/65] step:[1200/2202] loss:3.592268 took:0.10837s\n",
      "Epoch[35/65] step:[1400/2202] loss:3.922460 took:0.11438s\n",
      "Epoch[35/65] step:[1600/2202] loss:3.820652 took:0.11854s\n",
      "Epoch[35/65] step:[1800/2202] loss:3.570054 took:0.10902s\n"
     ]
    }
   ],
   "source": [
    "###============= train\n",
    "n_epoch = 65\n",
    "for epoch in range(n_epoch):\n",
    "    epoch_time = time.time()\n",
    "    ## shuffle training data\n",
    "    from sklearn.utils import shuffle\n",
    "    trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "    ## train an epoch\n",
    "    total_err, n_iter = 0, 0\n",
    "    for X, Y in tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False):\n",
    "        step_time = time.time()\n",
    "\n",
    "        X = tl.prepro.pad_sequences(X)\n",
    "        _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "        _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n",
    "\n",
    "        _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "        _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n",
    "        _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
    "\n",
    "        ## you can view the data here\n",
    "        # for i in range(len(X)):\n",
    "        #     print(i, [idx2word[id] for id in X[i]])\n",
    "        #     # print(i, [idx2word[id] for id in Y[i]])\n",
    "        #     print(i, [idx2word[id] for id in _target_seqs[i]])\n",
    "        #     print(i, [idx2word[id] for id in _decode_seqs[i]])\n",
    "        #     print(i, _target_mask[i])\n",
    "        #     print(len(_target_seqs[i]), len(_decode_seqs[i]), len(_target_mask[i]))\n",
    "        # exit()\n",
    "\n",
    "        _, err = sess.run([train_op, loss],\n",
    "                        {encode_seqs: X,\n",
    "                        decode_seqs: _decode_seqs,\n",
    "                        target_seqs: _target_seqs,\n",
    "                        target_mask: _target_mask})\n",
    "\n",
    "        if n_iter % 200 == 0:\n",
    "            print(\"Epoch[%d/%d] step:[%d/%d] loss:%f took:%.5fs\" % (epoch, n_epoch, n_iter, n_step, err, time.time() - step_time))\n",
    "\n",
    "        total_err += err; n_iter += 1\n",
    "\n",
    "        ###============= inference\n",
    "        if n_iter % 1000 == 0:\n",
    "            seeds = [\"happy birthday have a nice day\",\n",
    "                    \"how was it going\"]\n",
    "            for seed in seeds:\n",
    "                print(\"Query >\", seed)\n",
    "                seed_id = [word2idx[w] for w in seed.split(\" \")]\n",
    "                for _ in range(5):  # 1 Query --> 5 Reply\n",
    "                    # 1. encode, get state\n",
    "                    state = sess.run(net_rnn.final_state_encode,\n",
    "                                    {encode_seqs2: [seed_id]})\n",
    "                    # 2. decode, feed start_id, get first word\n",
    "                    #   ref https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py\n",
    "                    o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "                                    {net_rnn.initial_state_decode: state,\n",
    "                                    decode_seqs2: [[start_id]]})\n",
    "                    w_id = tl.nlp.sample_top(o[0], top_k=3)\n",
    "                    w = idx2word[w_id]\n",
    "                    # 3. decode, feed state iteratively\n",
    "                    sentence = [w]\n",
    "                    for _ in range(30): # max sentence length\n",
    "                        o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "                                        {net_rnn.initial_state_decode: state,\n",
    "                                        decode_seqs2: [[w_id]]})\n",
    "                        w_id = tl.nlp.sample_top(o[0], top_k=2)\n",
    "                        w = idx2word[w_id]\n",
    "                        if w_id == end_id:\n",
    "                            break\n",
    "                        sentence = sentence + [w]\n",
    "                    print(\" >\", ' '.join(sentence))\n",
    "\n",
    "    print(\"Epoch[%d/%d] averaged loss:%f took:%.5fs\" % (epoch, n_epoch, total_err/n_iter, time.time()-epoch_time))\n",
    "\n",
    "    tl.files.save_npz(net.all_params, name='n2.npz', sess=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
